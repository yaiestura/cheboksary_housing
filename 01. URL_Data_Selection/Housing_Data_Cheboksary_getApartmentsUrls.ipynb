{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 262 pages of apartments on the website overall\n"
     ]
    }
   ],
   "source": [
    "# Basic Website URL\n",
    "root_url = 'https://cheb.ws/prodam/kvartira/'\n",
    "\n",
    "# Get HTML Contents of the webpage via HTTP Request\n",
    "try:\n",
    "    r = requests.get(root_url, timeout=3)\n",
    "    r.raise_for_status()\n",
    "except requests.exceptions.HTTPError as err:\n",
    "    print ('HTTP Error:', err)\n",
    "except requests.exceptions.ConnectionError as err:\n",
    "    print ('Error Connecting:', err)\n",
    "except requests.exceptions.Timeout as err:\n",
    "    print ('Timeout Error:', err)\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print ('Oops, Unknown Error', err)\n",
    "\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "# Select specific div with page selector class\n",
    "pages_divs = soup.find_all('div', class_='link_bar')[0]\n",
    "\n",
    "# Select href attributes of links\n",
    "hrefs = [a['href'] for a in pages_divs.find_all('a')]\n",
    "hrefs.pop()\n",
    "\n",
    "# Find overall number of pages to create array of URLs\n",
    "num_of_pages = int(re.findall(r'\\d+', hrefs[-1])[0])\n",
    "print(f'There are {num_of_pages + 1} pages of apartments on the website overall')\n",
    "\n",
    "# Fill pages array with URL values to process data later\n",
    "pages = [root_url + f'?page={page}' for page in range(1, num_of_pages + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35 s, sys: 470 ms, total: 35.5 s\n",
      "Wall time: 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "apartments = []\n",
    "\n",
    "# Process and retreive URL data in a loop\n",
    "\n",
    "for page in pages:\n",
    "\n",
    "    r = requests.get(page)\n",
    "    \n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', {'class': 'map'})\n",
    "    rows = table.find_all('tr')[1:]\n",
    "    \n",
    "    for row in rows:\n",
    "        \n",
    "        link = row.find('td', {'class': 'col-type'}).find('a', {'class': 'titleline'})['href']\n",
    "        apartments.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select unique URLs\n",
    "\n",
    "apartments = list(set(apartments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write links to a CSV file\n",
    "\n",
    "with open('apartments_links.csv', 'w') as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_ALL, delimiter='\\n')\n",
    "    writer.writerow(apartments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
